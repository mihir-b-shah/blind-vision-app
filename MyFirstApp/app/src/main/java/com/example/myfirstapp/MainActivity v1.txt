package com.example.myfirstapp;

import android.content.Context;
import android.content.Intent;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Point;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Bundle;
import android.os.Environment;
import android.provider.MediaStore;
import android.speech.RecognizerIntent;
import android.speech.tts.TextToSpeech;
import android.speech.tts.UtteranceProgressListener;
import android.support.annotation.NonNull;
import android.support.v4.content.FileProvider;
import android.support.v7.app.AppCompatActivity;
import android.util.DisplayMetrics;
import android.util.SparseArray;
import android.view.View;
import android.widget.Toast;

import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.android.gms.vision.Frame;
import com.google.android.gms.vision.text.TextBlock;
import com.google.android.gms.vision.text.TextRecognizer;
import com.google.api.client.extensions.android.json.AndroidJsonFactory;
import com.google.api.client.http.javanet.NetHttpTransport;
import com.google.api.services.vision.v1.Vision;
import com.google.api.services.vision.v1.VisionRequestInitializer;
import com.google.api.services.vision.v1.model.AnnotateImageRequest;
import com.google.api.services.vision.v1.model.AnnotateImageResponse;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesResponse;
import com.google.api.services.vision.v1.model.Block;
import com.google.api.services.vision.v1.model.BoundingPoly;
import com.google.api.services.vision.v1.model.Feature;
import com.google.api.services.vision.v1.model.Image;
import com.google.api.services.vision.v1.model.Page;
import com.google.api.services.vision.v1.model.Paragraph;
import com.google.api.services.vision.v1.model.Symbol;
import com.google.api.services.vision.v1.model.TextAnnotation;
import com.google.api.services.vision.v1.model.Vertex;
import com.google.api.services.vision.v1.model.Word;
import com.google.firebase.ml.vision.FirebaseVision;
import com.google.firebase.ml.vision.common.FirebaseVisionImage;
import com.google.firebase.ml.vision.label.FirebaseVisionLabel;
import com.google.firebase.ml.vision.label.FirebaseVisionLabelDetector;
import com.google.firebase.ml.vision.label.FirebaseVisionLabelDetectorOptions;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Locale;

public class MainActivity extends AppCompatActivity implements SensorEventListener, AsyncResponse<AnnotateImageResponse> {

    TextToSpeech tts;
    public static final int SPEECH_REQUEST_CODE = 0;
    public static final int REQUEST_IMAGE_CAPTURE = 1;
    public static final int REQUEST_TAKE_PHOTO = 1;
    private static String spkText;
    private String mCurrentPhotoPath;
    private SensorManager mSensorManager;
    private SparseArray<TextBlock> arr;
    private String[] tb_values;
    private Sensor[] sensors = new Sensor[2];
    private float[] magData;
    private float[] accData;
    private double deflect_X;
    private double deflect_Y;
    private static final double VIEW_ANGLE = Math.acos((35.5 * 35.5 - 57.25 * 57.25 - 56.25 * 56.25) / (-2 * 57.25 * 56.25));
    private static final double NEGLIGIBLE_ANGLE = 0.01;
    private boolean runnable = true;
    public static final int SHIFT = 24;
    private int nlp_result;
    private Bitmap b;
    private TextAnnotation vision_result;
    private List<AnnotateImageResponse> list;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        tts = new TextToSpeech(getApplicationContext(), new TextToSpeech.OnInitListener() {
            @Override
            public void onInit(int status) {
                if (status != TextToSpeech.ERROR)
                    tts.setLanguage(Locale.US);
            }
        });

        tts.setOnUtteranceProgressListener(new UtteranceProgressListener() {

            @Override
            public void onStart(String utteranceId) {
            }

            @Override
            public void onDone(String utteranceId) {

                if (utteranceId.equals("error")) {
                    dispatchTakePictureIntent();
                } else if (utteranceId.equals("out"))
                    next();

            }

            @Override
            public void onError(String utteranceId) {
            }
        });

        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);
        sensors[0] = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD);
        sensors[1] = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);
        mSensorManager.registerListener(this, sensors[0], SensorManager.SENSOR_DELAY_NORMAL);
        mSensorManager.registerListener(this, sensors[1], SensorManager.SENSOR_DELAY_NORMAL);
    }

    private void displaySpeechRecognizer() {
        Intent intent = new Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH);
        intent.putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL, RecognizerIntent.LANGUAGE_MODEL_FREE_FORM);
        // Start the activity, the intent will be populated with the speech text
        startActivityForResult(intent, SPEECH_REQUEST_CODE);
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {

        if (requestCode == SPEECH_REQUEST_CODE && resultCode == RESULT_OK) {
            ArrayList<String> results = data.getStringArrayListExtra(RecognizerIntent.EXTRA_RESULTS);
            spkText = results.get(0);
            super.onActivityResult(requestCode, resultCode, data);
            dispatchTakePictureIntent();
        }

        if (requestCode == REQUEST_IMAGE_CAPTURE && resultCode == RESULT_OK) {
            CallAPI asynctask = new CallAPI();
            asynctask.deleg = this;
            asynctask.execute(mCurrentPhotoPath, BitmapFactory.decodeFile(mCurrentPhotoPath));
        }
    }

    private void dispatchTakePictureIntent() {
        Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);

        if (takePictureIntent.resolveActivity(getPackageManager()) != null) {
            // Create the File where the photo should go
            File photoFile = null;
            try {
                photoFile = createImageFile();
            } catch (IOException ex) {

            }

            // Continue only if the File was successfully created
            if (photoFile != null) {
                Uri photoURI = FileProvider.getUriForFile(this,
                        "com.example.android.fileprovider",
                        photoFile);

                takePictureIntent.putExtra(MediaStore.EXTRA_OUTPUT, photoURI);
                // setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_);
                startActivityForResult(takePictureIntent, REQUEST_TAKE_PHOTO);
            }
        }
    }

    private File createImageFile() throws IOException {
        String imageFileName = "JPEG_test";
        File storageDir = getExternalFilesDir(Environment.DIRECTORY_PICTURES);
        File image = File.createTempFile(
                imageFileName,  /* prefix */
                ".jpg",         /* suffix */
                storageDir      /* directory */
        );

        // Save a file: path for use with ACTION_VIEW intents
        mCurrentPhotoPath = image.getAbsolutePath();
        return image;
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();

        for (int i = 0; i < sensors.length; i++) {
            mSensorManager.unregisterListener(this, sensors[i]);
        }
    }

    public void respond(View v) {
        // Convert commands to text
        // Native command in Java
        displaySpeechRecognizer();

        // Take the photo
    }

    // https://code.tutsplus.com/tutorials/how-to-use-the-google-cloud-vision-api-in-android-apps--cms-29009

    public void tempContinueProgramV2(List<AnnotateImageResponse> annotations) {

        // Need to multithread
        // Doing too much work on the main thread

        List<Page> pages;

        Paragraph max = null;
        Page select = null;
        double max_score = 0;

        System.out.println(227);

        if(!annotations.isEmpty()) {
            for (AnnotateImageResponse resp : annotations) {

                System.out.println(resp.getLabelAnnotations());
                System.out.println(resp.getTextAnnotations());
                System.out.println(resp.getFullTextAnnotation());

                pages = resp.getFullTextAnnotation().getPages();
                List<Block> blocks;

                for (Page pg : pages) {

                    blocks = pg.getBlocks();
                    List<Paragraph> paragraphs;

                    for (Block b : blocks) {

                        paragraphs = b.getParagraphs();
                        List<Word> words;

                        for (Paragraph p : paragraphs) {

                            words = p.getWords();
                            List<Symbol> symbols;
                            StringBuilder text = new StringBuilder();

                            for (Word w : words) {
                                symbols = w.getSymbols();

                                for (Symbol s : symbols) {
                                    text.append(s.getText());
                                    System.out.println(s.getText());
                                }

                                text.append(' ');
                            }

                            System.out.println(text);
                            System.out.println(spkText);

                            double val = compare(text, spkText);

                            if (Double.compare(val, max_score) > 0) {
                                max_score = val;
                                max = p;
                                select = pg;
                            }

                        }

                    }
                }
            }

            BoundingPoly box = max.getBoundingBox();

            int Xmin = select.getWidth();
            int Ymin = select.getHeight();
            int Xmax = 0;
            int Ymax = 0;

            for (Vertex v : box.getVertices()) {
                if (v.getX() > Xmax)
                    Xmax = v.getX();
                if (v.getY() > Ymax)
                    Ymax = v.getY();
                if (v.getX() < Xmin)
                    Xmin = v.getX();
                if (v.getY() < Ymin)
                    Ymin = v.getY();
            }

            deflect_X = (double) (Xmax + Xmin) / 2;
            deflect_Y = (double) (Ymax + Ymin) / 2;

            // navigate();
        }
    }

    private double compare(StringBuilder text, String spkText) {

        return Double.NaN;
    }

    public void tempContinueProgram() {

        Frame.Builder fr = new Frame.Builder();
        b = BitmapFactory.decodeFile(mCurrentPhotoPath);

        try {
            fr.setBitmap(b);
        } catch (NullPointerException e) {

            String err_message = "A photo was not taken properly. Please try again.";
            Toast.makeText(getApplicationContext(), err_message, Toast.LENGTH_LONG).show();
            tts.speak(err_message, TextToSpeech.QUEUE_ADD, null, "error");
        }

        Frame frame = fr.build();
        TextRecognizer.Builder tr = new TextRecognizer.Builder(getApplicationContext());
        TextRecognizer tre = tr.build();

        arr = tre.detect(frame);
        tb_values = new String[arr.size()];
        System.out.println(arr.size());

        for (int i = 0; i < arr.size(); i++) {
            tb_values[i] = arr.get(i).getValue();
            System.out.println(tb_values[i]);
            Toast.makeText(getApplicationContext(), tb_values[i], Toast.LENGTH_LONG).show();
            tts.speak(tb_values[i], TextToSpeech.QUEUE_ADD, null, "out");
        }
    }

    public void next() {
        // https://firebase.google.com/docs/ml-kit/android/label-images?authuser=3
        FirebaseVisionLabelDetectorOptions opt = new FirebaseVisionLabelDetectorOptions.Builder().setConfidenceThreshold(0.5f).build();
        FirebaseVisionImage image = FirebaseVisionImage.fromBitmap(b);

        System.out.println(b.getWidth());
        FirebaseVisionLabelDetector detector = FirebaseVision.getInstance().getVisionLabelDetector(opt);

        Task<List<FirebaseVisionLabel>> result =
                detector.detectInImage(image)
                        .addOnSuccessListener(
                                new OnSuccessListener<List<FirebaseVisionLabel>>() {
                                    @Override
                                    public void onSuccess(List<FirebaseVisionLabel> labels) {
                                        String[] lbls = new String[labels.size()];

                                        for (int i = 0; i < labels.size(); i++) {
                                            lbls[i] = labels.get(i).getLabel();
                                            Toast.makeText(getApplicationContext(), lbls[i], Toast.LENGTH_LONG).show();
                                            tts.speak(lbls[i], TextToSpeech.QUEUE_ADD, null);

                                            nlp_result = textMatch(spkText, tb_values, lbls);
                                        }
                                    }
                                })
                        .addOnFailureListener(
                                new OnFailureListener() {
                                    @Override
                                    public void onFailure(@NonNull Exception e) {
                                        android.os.Process.killProcess(android.os.Process.myPid());
                                        System.exit(1);
                                    }
                                });

        DisplayMetrics dm = new DisplayMetrics();
        getWindowManager().getDefaultDisplay().getMetrics(dm);

        int mid_X = dm.widthPixels / 2;
        int mid_Y = dm.heightPixels / 2;
        int index = -1;

        if ((nlp_result & 1 << 23) == 0) {
            String message = "No text labels were found for navigation purposes.";
            Toast.makeText(getApplicationContext(), message, Toast.LENGTH_LONG).show();
            tts.speak(message, TextToSpeech.QUEUE_ADD, null);
        } else {
            index = nlp_result % (1 << 22);
        }

        String str = arr.get(index).getValue();
        Point[] points = arr.get(index).getCornerPoints();

        int X = (points[0].x + points[1].x) / 2;
        int Y = (points[0].y + points[1].y) / 2;

        deflect_X = (X - mid_X) / mid_X;
        deflect_Y = (Y - mid_Y) / mid_Y;

        navigate();
    }

    @Override
    public void onSensorChanged(SensorEvent event) {

        if (runnable) {

            int type = event.sensor.getType();

            if (type == Sensor.TYPE_ACCELEROMETER) {
                accData = event.values.clone();
            } else if (type == Sensor.TYPE_MAGNETIC_FIELD) {
                magData = event.values.clone();
            }

            runnable = false;
        }
    }

    /**
     * NAVIGATION INFORMATION
     * <p>
     * 1. Location within the photo frame
     * 2. Azimuth, pitch, and roll
     * <p>
     * AZIMUTH: Angle between device y axis, along long side of the phone and the magnetic north pole
     * PITCH: Angle between a plane parallel to the device screen and one parallel to the ground
     * ROLL: Angle between a plane parallel to device screen and one normal to the ground
     */

    public void navigate() {

        // https://google-developer-training.gitbooks.io/android-developer-advanced-course-concepts/content/unit-1-expand-the-user-experience/lesson-3-sensors/3-2-c-motion-and-position-sensors/3-2-c-motion-and-position-sensors.html#devicerotation
        System.out.println("GOT TO NAVIGATE");

        float[] rotationMatrix = new float[9];
        boolean canRotate = SensorManager.getRotationMatrix(rotationMatrix, null, accData, magData);

        float[] orientation_angles = new float[3];

        if (canRotate) {
            SensorManager.getOrientation(rotationMatrix, orientation_angles);
        }

        // Add adaptive functionality

        float AZIMUTH = orientation_angles[0];

        float ORIG_AZIMUTH = AZIMUTH;

        float PITCH = orientation_angles[1];
        float ROLL = orientation_angles[2];

        // Find the normal vector of the plane
        // Vector is <-tan(PITCH), 1> in 2D space

        double[] normal_vector = new double[3];

        normal_vector[0] = -Math.tan(PITCH) * Math.cos(ROLL);
        normal_vector[1] = -Math.tan(PITCH) * Math.sin(ROLL);
        normal_vector[2] = 1;

        double angle = Math.atan(deflect_X * Math.tan(VIEW_ANGLE / 2));

        runnable = true;
        if (angle > NEGLIGIBLE_ANGLE) {
            while (AZIMUTH - ORIG_AZIMUTH < angle) ;
        } else if (angle < -NEGLIGIBLE_ANGLE) {
            while (ORIG_AZIMUTH - AZIMUTH > angle) ;
        }
    }

    @Override
    public void onAccuracyChanged(Sensor sensor, int accuracy) {
        // Unused
    }

    private int textMatch(String spkText, String[] tb_values, String[] lbls) {

        /** Specification for how to return
         *
         * Shift 24: which array
         * Shift 23: if located
         * Shift 0: Reference + 1
         * Return -1 if no matches found
         */

        return (1 << 24) + (1 << 23) + 1;
    }

    @Override
    public void finish(List<AnnotateImageResponse> list) {
        tempContinueProgramV2(list);
    }

    // https://stackoverflow.com/questions/12575068/how-to-get-the-result-of-onpostexecute-to-main-activity-because-asynctask-is-a
    private class CallAPI extends AsyncTask<Object, Integer, String> {
    // Do the long-running work in here

        private AsyncResponse deleg = null;
        private List<AnnotateImageResponse> annotations;

        @Override
        protected String doInBackground(Object... params) {

            System.out.println(500);

            Vision.Builder vb = new Vision.Builder(new NetHttpTransport(), new AndroidJsonFactory(), null);
            vb.setVisionRequestInitializer(new VisionRequestInitializer(    "AIzaSyDnCi0AaR5qNAc6S5gkGWfpn0XwgHJg1zc"));
            vb.setApplicationName("MyFirstApp");

            Vision vision = vb.build();
            Feature df = new Feature();
            df.setType("TEXT_DETECTION");

                /*
                params[0] = absolute path of the photo
                params[1] = bitmap
                 */

            try {

                Bitmap b = (Bitmap) params[1];
                BitmapFactory.Options options = new BitmapFactory.Options();
                options.inSampleSize = 8;
                b = Bitmap.createScaledBitmap((Bitmap) b, 768, 1024, true);

                ByteArrayOutputStream stream = new ByteArrayOutputStream();
                b.compress(Bitmap.CompressFormat.JPEG, 70, stream);
                byte[] data = stream.toByteArray();

                Image img = new Image();
                img.encodeContent(data);
                AnnotateImageRequest request = new AnnotateImageRequest();
                request.setImage(img);
                request.setFeatures(Arrays.asList(df));
                BatchAnnotateImagesRequest batchRequest = new BatchAnnotateImagesRequest();

                batchRequest.setRequests(Arrays.asList(request));
	
                Vision.Images vi = vision.images();
                Vision.Images.Annotate via = vi.annotate(batchRequest);

                System.out.println(via.getKey());
                System.out.println(via.getAccessToken());
                System.out.println(via.getOauthToken());

                BatchAnnotateImagesResponse batchResponse = via.execute();

                annotations = batchResponse.getResponses();

            } catch (IOException e) {
                e.printStackTrace();
            }

            return ""; // change
        }

        @Override
        protected void onPostExecute(String result) {
            deleg.finish(annotations);
        }
    }
}