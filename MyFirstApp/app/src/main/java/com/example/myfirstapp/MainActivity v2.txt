package com.example.myfirstapp;

import android.content.Context;
import android.content.Intent;
import android.content.pm.ActivityInfo;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Bundle;
import android.os.Environment;
import android.provider.MediaStore;
import android.speech.RecognizerIntent;
import android.speech.tts.TextToSpeech;
import android.speech.tts.UtteranceProgressListener;
import android.support.v4.content.FileProvider;
import android.support.v7.app.AppCompatActivity;
import android.util.SparseArray;
import android.view.View;

import com.google.android.gms.vision.text.TextBlock;
import com.google.api.client.extensions.android.json.AndroidJsonFactory;
import com.google.api.client.http.javanet.NetHttpTransport;
import com.google.api.services.vision.v1.Vision;
import com.google.api.services.vision.v1.VisionRequestInitializer;
import com.google.api.services.vision.v1.model.AnnotateImageRequest;
import com.google.api.services.vision.v1.model.AnnotateImageResponse;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesResponse;
import com.google.api.services.vision.v1.model.Block;
import com.google.api.services.vision.v1.model.BoundingPoly;
import com.google.api.services.vision.v1.model.EntityAnnotation;
import com.google.api.services.vision.v1.model.Feature;
import com.google.api.services.vision.v1.model.Image;
import com.google.api.services.vision.v1.model.Page;
import com.google.api.services.vision.v1.model.Paragraph;
import com.google.api.services.vision.v1.model.Symbol;
import com.google.api.services.vision.v1.model.TextAnnotation;
import com.google.api.services.vision.v1.model.Vertex;
import com.google.api.services.vision.v1.model.Word;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Locale;

public class MainActivity extends AppCompatActivity implements SensorEventListener, AsyncResponse<AnnotateImageResponse> {

    TextToSpeech tts;
    public static final int SPEECH_REQUEST_CODE = 0;
    public static final int REQUEST_IMAGE_CAPTURE = 1;
    public static final int REQUEST_TAKE_PHOTO = 1;
    private String secPath;
    private static String spkText;
    private String mCurrentPhotoPath;
    private SensorManager mSensorManager;
    private SparseArray<TextBlock> arr;
    private String[] tb_values;
    private Sensor[] sensors = new Sensor[2];
    private float[] magData;
    private float[] accData;
    public static int WIDTH = 1024;
    public static int HEIGHT = 768;
    private double deflect_X;
    private double deflect_Y;
    private static final double VIEW_ANGLE = Math.acos((35.5 * 35.5 - 57.25 * 57.25 - 56.25 * 56.25) / (-2 * 57.25 * 56.25));
    private static final double NEGLIGIBLE_ANGLE = 0.01;
    private boolean runnable = true;
    public static final int SHIFT = 24;
    private int nlp_result;
    private Bitmap b;
    private TextAnnotation vision_result;
    private List<AnnotateImageResponse> list;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        tts = new TextToSpeech(getApplicationContext(), new TextToSpeech.OnInitListener() {
            @Override
            public void onInit(int status) {
                if (status != TextToSpeech.ERROR)
                    tts.setLanguage(Locale.US);
            }
        });

        tts.setOnUtteranceProgressListener(new UtteranceProgressListener() {

            @Override
            public void onStart(String utteranceId) {
            }

            @Override
            public void onDone(String utteranceId) {

                if (utteranceId.equals("error")) {
                    dispatchTakePictureIntent();
                } else if (utteranceId.equals("out")) {
                    //
                }
            }

            @Override
            public void onError(String utteranceId) {
            }
        });

        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);
        sensors[0] = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD);
        sensors[1] = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);
        mSensorManager.registerListener(this, sensors[0], SensorManager.SENSOR_DELAY_NORMAL);
        mSensorManager.registerListener(this, sensors[1], SensorManager.SENSOR_DELAY_NORMAL);
    }

    private void displaySpeechRecognizer() {
        Intent intent = new Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH);
        intent.putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL, RecognizerIntent.LANGUAGE_MODEL_FREE_FORM);
        // Start the activity, the intent will be populated with the speech text
        startActivityForResult(intent, SPEECH_REQUEST_CODE);
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {

        if (requestCode == SPEECH_REQUEST_CODE && resultCode == RESULT_OK) {
            ArrayList<String> results = data.getStringArrayListExtra(RecognizerIntent.EXTRA_RESULTS);
            spkText = results.get(0);
            super.onActivityResult(requestCode, resultCode, data);
            dispatchTakePictureIntent();
        }

        if (requestCode == REQUEST_IMAGE_CAPTURE && resultCode == RESULT_OK) {
            System.out.println("Line 140: " + mCurrentPhotoPath);
            System.out.println("Line 140: " + secPath);
            CallAPI asynctask = new CallAPI();
            asynctask.deleg = this;
            asynctask.execute(mCurrentPhotoPath, BitmapFactory.decodeFile(mCurrentPhotoPath));
        }
    }

    @Override
    protected void onSaveInstanceState(Bundle b) {
        super.onSaveInstanceState(b);
        b.putString("path", mCurrentPhotoPath);
    }

    @Override
    protected void onRestoreInstanceState(Bundle saved) {
        super.onRestoreInstanceState(saved);
        mCurrentPhotoPath = saved.getString("path");
    }

    private void dispatchTakePictureIntent() {
        Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);

        if (takePictureIntent.resolveActivity(getPackageManager()) != null) {
            // Create the File where the photo should go
            File photoFile = null;
            try {
                System.out.println(153);
                photoFile = createImageFile();
                System.out.println(mCurrentPhotoPath);
            } catch (IOException ex) {

            }

            // Continue only if the File was successfully created
            if (photoFile != null) {
                Uri photoURI = FileProvider.getUriForFile(this,
                        "com.example.android.fileprovider",
                        photoFile);

                takePictureIntent.putExtra(MediaStore.EXTRA_OUTPUT, photoURI);
                setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);
                System.out.println("Line 169: " + mCurrentPhotoPath);
                secPath = new String(mCurrentPhotoPath);
                System.out.println("Line 169: " + secPath);
                startActivityForResult(takePictureIntent, REQUEST_TAKE_PHOTO);
            }
        }
    }

    private File createImageFile() throws IOException {
        String imageFileName = "JPEG_test";
        File storageDir = getExternalFilesDir(Environment.DIRECTORY_PICTURES);
        File image = File.createTempFile(
                imageFileName,  /* prefix */
                ".jpg",         /* suffix */
                storageDir      /* directory */
        );

        System.out.println(181);
        System.out.println(image);

        // Save a file: path for use with ACTION_VIEW intents
        mCurrentPhotoPath = image.getAbsolutePath();
        System.out.println(mCurrentPhotoPath);

        return image;
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();

        for (int i = 0; i < sensors.length; i++) {
            mSensorManager.unregisterListener(this, sensors[i]);
        }
    }

    public void respond(View v) {
        // Convert commands to text
        // Native command in Java
        displaySpeechRecognizer();

        // Take the photo
    }

    // https://code.tutsplus.com/tutorials/how-to-use-the-google-cloud-vision-api-in-android-apps--cms-29009

    public void tempContinueProgramV2(List<AnnotateImageResponse> annotations) {

        // Need to multithread
        // Doing too much work on the main thread

        List<Page> pages;
        BoundingPoly opt = null;
        String descr;
        float confidence = 0.0f;

        System.out.println(227);

        if(!annotations.isEmpty()) {
            for (AnnotateImageResponse resp : annotations) {


                List<EntityAnnotation> logos = resp.getLogoAnnotations();

                for(EntityAnnotation ea: logos) {
                    String name = ea.getDescription();
                    float score = ea.getScore();

                    float conf_loc = genScore(score, compare(name, spkText));

                    if(conf_loc > confidence && compare(name, spkText) == 1.0) {
                        opt = ea.getBoundingPoly();
                        descr = name;
                        confidence = conf_loc;
                    }
                }

                pages = resp.getFullTextAnnotation().getPages();
                List<Block> blocks;

                for (Page pg : pages) {

                    blocks = pg.getBlocks();
                    List<Paragraph> paragraphs;

                    for (Block b : blocks) {

                        paragraphs = b.getParagraphs();
                        List<Word> words;

                        for (Paragraph p : paragraphs) {

                            words = p.getWords();
                            List<Symbol> symbols;
                            StringBuilder text = new StringBuilder();

                            for (Word w : words) {
                                symbols = w.getSymbols();

                                for (Symbol s : symbols) {
                                    text.append(s.getText());
                                }

                                text.append(' ');
                            }

                            System.out.println(text);
                            System.out.println(spkText);

                            double val = compare(text.toString(), spkText);

                            if (Double.compare(val, confidence) > 0) {
                                confidence = (float) val;
                                opt = p.getBoundingBox();
                                descr = text.toString();
                            }

                        }

                    }
                }
            }

            int Xmin = WIDTH;
            int Ymin = HEIGHT;
            int Xmax = 0;
            int Ymax = 0;

            for (Vertex v : opt.getVertices()) {
                if (v.getX() > Xmax)
                    Xmax = v.getX();
                if (v.getY() > Ymax)
                    Ymax = v.getY();
                if (v.getX() < Xmin)
                    Xmin = v.getX();
                if (v.getY() < Ymin)
                    Ymin = v.getY();
            }

            deflect_X = (double) (Xmax + Xmin) / 2;
            deflect_Y = (double) (Ymax + Ymin) / 2;

            navigate();
        }
    }

    private float genScore(float score, double compare) {
        return score*(float)compare;
    }

    private double compare(String text, String spkText) {

        if(text.equals(spkText))
            return 1.0;
        else
            return 0.0;
    }

    @Override
    public void onSensorChanged(SensorEvent event) {

        if (runnable) {

            int type = event.sensor.getType();

            if (type == Sensor.TYPE_ACCELEROMETER) {
                accData = event.values.clone();
            } else if (type == Sensor.TYPE_MAGNETIC_FIELD) {
                magData = event.values.clone();
            }

            runnable = false;
        }
    }

    /**
     * NAVIGATION INFORMATION
     * <p>
     * 1. Location within the photo frame
     * 2. Azimuth, pitch, and roll
     * <p>
     * AZIMUTH: Angle between device y axis, along long side of the phone and the magnetic north pole
     * PITCH: Angle between a plane parallel to the device screen and one parallel to the ground
     * ROLL: Angle between a plane parallel to device screen and one normal to the ground
     */

    public void navigate() {

        // https://google-developer-training.gitbooks.io/android-developer-advanced-course-concepts/content/unit-1-expand-the-user-experience/lesson-3-sensors/3-2-c-motion-and-position-sensors/3-2-c-motion-and-position-sensors.html#devicerotation
        System.out.println("GOT TO NAVIGATE");

        float[] rotationMatrix = new float[9];
        boolean canRotate = SensorManager.getRotationMatrix(rotationMatrix, null, accData, magData);

        float[] orientation_angles = new float[3];

        if (canRotate) {
            SensorManager.getOrientation(rotationMatrix, orientation_angles);
        }

        // Add adaptive functionality

        float AZIMUTH = orientation_angles[0];

        float ORIG_AZIMUTH = AZIMUTH;

        float PITCH = orientation_angles[1];
        float ROLL = orientation_angles[2];

        // Find the normal vector of the plane
        // Vector is <-tan(PITCH), 1> in 2D space

        double[] normal_vector = new double[3];

        normal_vector[0] = -Math.tan(PITCH) * Math.cos(ROLL);
        normal_vector[1] = -Math.tan(PITCH) * Math.sin(ROLL);
        normal_vector[2] = 1;

        double angle = Math.atan(deflect_X * Math.tan(VIEW_ANGLE / 2));

        runnable = true;
        if (angle > NEGLIGIBLE_ANGLE) {
            while (AZIMUTH - ORIG_AZIMUTH < angle) ;
        } else if (angle < -NEGLIGIBLE_ANGLE) {
            while (ORIG_AZIMUTH - AZIMUTH > angle) ;
        }
    }

    @Override
    public void onAccuracyChanged(Sensor sensor, int accuracy) {
        // Unused
    }

    @Override
    public void finish(List<AnnotateImageResponse> list) {
        tempContinueProgramV2(list);
    }

    // https://stackoverflow.com/questions/12575068/how-to-get-the-result-of-onpostexecute-to-main-activity-because-asynctask-is-a
    private class CallAPI extends AsyncTask<Object, Integer, String> {
        // Do the long-running work in here

        private AsyncResponse deleg = null;
        private List<AnnotateImageResponse> annotations;
        private List<AnnotateImageResponse> second;

        @Override
        protected String doInBackground(Object... params) {

            System.out.println(500);

            Vision.Builder vb = new Vision.Builder(new NetHttpTransport(), new AndroidJsonFactory(), null);
            vb.setVisionRequestInitializer(new VisionRequestInitializer(    "AIzaSyDnCi0AaR5qNAc6S5gkGWfpn0XwgHJg1zc"));
            vb.setApplicationName("MyFirstApp");

            Vision vision = vb.build();
            Feature df = new Feature();
            df.setType("TEXT_DETECTION");
            Feature df2 = new Feature();
            df2.setType("LOGO_DETECTION");
            Feature df3 = new Feature();
            df3.setType("OBJECT_LOCALIZATION");

                /*
                params[0] = absolute path of the photo
                params[1] = bitmap
                 */

            try {

                Bitmap b = (Bitmap) params[1];

                System.out.println(mCurrentPhotoPath);
                System.out.println(b);

                b = Bitmap.createScaledBitmap((Bitmap) b, HEIGHT, WIDTH, true);

                ByteArrayOutputStream stream = new ByteArrayOutputStream();
                b.compress(Bitmap.CompressFormat.JPEG, 70, stream);
                byte[] data = stream.toByteArray();

                Image img = new Image();
                img.encodeContent(data);
                AnnotateImageRequest request = new AnnotateImageRequest();
                request.setImage(img);
                request.setFeatures(Arrays.asList(df, df2, df3));
                BatchAnnotateImagesRequest batchRequest = new BatchAnnotateImagesRequest();

                batchRequest.setRequests(Arrays.asList(request));

                Vision.Images vi = vision.images();
                Vision.Images.Annotate via = vi.annotate(batchRequest);

                BatchAnnotateImagesResponse batchResponse = via.execute();
                annotations = batchResponse.getResponses();

            } catch (IOException e) {
                e.printStackTrace();
            }

            return ""; // change
        }

        @Override
        protected void onPostExecute(String result) {
            deleg.finish(annotations);
        }
    }
}